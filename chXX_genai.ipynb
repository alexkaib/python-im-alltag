{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01904701-3cbe-4819-bf6a-93ad0467e0f4",
   "metadata": {},
   "source": [
    "# Teil XX: Generative KI einbinden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea260e-a072-4c5f-a01e-297307073d73",
   "metadata": {},
   "source": [
    "Aktuelle KI-Systeme basieren meist auf Algorithmen, die durch Wahrscheinlichkeitsberechnungen neue Texte, Sprache, Bilder oder Videos produzieren. Aufgrund ihrer probabilistischen Natur sind sie kein gutes Werkzeug, um regelmäßige, mehrschrittige Arbeitsabläufe zu digitalisieren - aber an den richtigen Stellen eingesetzt können sie unsere Programme um Funktionen ergänzen, die mit reinem Python-Code nicht erreichbar sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f4034-4805-4d18-81a6-a4f8f2af48ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sprachmodelle verstehen und nutzen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8c5c2-80aa-4f53-a59b-12806d442c20",
   "metadata": {},
   "source": [
    "[Siehe Folien zu LLMs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c27f1-5f30-44df-b640-b14a27e214b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Nachrichten senden und empfangen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b7a2d-4874-4db3-b935-1ba9dd9e1033",
   "metadata": {},
   "source": [
    "Wir nutzen in diesem Kapitel die Python-Bibliothek [LiteLLM](https://www.litellm.ai/), um mit verschiedenen LLMs zu kommunizieren. Alternativ könnten wir auch direkt die APIs von (u.a.) OpenAI, Mistral oder Google nutzen, aber zum Experimentieren mit verschiedenen Modellen ist ein einheitliches Interface nützlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736b1c6-9d7c-4a87-aa7f-6a37f5223602",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd649a-266f-4ff2-807d-bc54afdfedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260c74d-82fe-4d4d-8b39-314f6208fd84",
   "metadata": {},
   "source": [
    "Um mit Cloud-basierten LLMs zu interagieren benötigen wir API-Schlüssel zur Authentifizierung. Die meisten APIs sind kostenpflichtig, aber einige Anbieter bieten (begrenzte) kostenlose Zugänge an - allerdings werden die gesendeten Nachrichten dann i.d.R. für das Training zukünftiger LLMs genutzt. In jedem Fall ist das Anlegen eines Nutzeraccounts verpflichtend, oft mit Telefonverifizierung.\n",
    "\n",
    "Wir nutzen hier die APIs von [Mistral](https://admin.mistral.ai/organization/api-keys) und [Google](https://aistudio.google.com/app/apikey). Du kannst im folgenden Code-Schnipsel deine eigenen Schlüssel einsetzen, um sie im Rest des Kapitels verwenden zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16b910-e65a-479f-8bce-72871455121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.environ.get(\"GEMINI_API_KEY\"):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY_HERE\"\n",
    "\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = \"YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b573a3-36fb-409f-94c3-871c32d1b6ea",
   "metadata": {},
   "source": [
    "LLMs generieren Text, indem sie eine wahrscheinliche Weiterführung eines Input-Strings (sog. **Prompts**) berechnen. Die meisten bekannten LLMs sind außerdem darauf trainiert, auf Prompts als Nachrichten in einem Chat zu reagieren, so dass wir Fragen an sie schicken und eine (einigermaßen) sinnvolle Antwort erwarten können.\n",
    "\n",
    "Wenn wir den Input-Text an das LLM unserer Wahl schicken, müssen wir neben dem eigentlichen Inhalt auch die **Rolle** spezifizieren, die dieser Text bei der Generierung einnehmen soll. Bei normalen Anfragen ist das die `user`-Rolle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579067a-1cda-4232-a191-bc8dc175e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Nachricht, die wir an das LLM schicken\n",
    "instruction = {\n",
    "    \"content\": \"Wer ist der aktuelle Bundeskanzler?\",\n",
    "    \"role\": \"user\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a15331-8c43-4ff7-bfe8-e89fc1884eed",
   "metadata": {},
   "source": [
    "Die `completion`-Methode von LiteLLM lässt uns (u.a.) das zu verwendene Modell und den Input-String spezifizieren. Sie liefert ein `ModelResponse`-Objekt, das den generierten Text und einige Metadaten enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9418096-f3af-4368-bb35-c14be54af594",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_response = completion(\n",
    "  model=\"gemini/gemini-2.5-flash\",\n",
    "  messages=[instruction]\n",
    ")\n",
    "\n",
    "print(gemini_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa41491-fb71-4aeb-bed8-8c0ab0b6e9b1",
   "metadata": {},
   "source": [
    "Durch den `model`-Parameter lässt sich leicht ein anderes LLM für dieselbe Aufgabe verwenden. Eine Liste der unterstützten Modelle bzw. Anbieter findet sich hier: https://docs.litellm.ai/docs/providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0a8a2-67a8-48c9-a6e3-7c4ba121b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_response = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[instruction]\n",
    ")\n",
    "\n",
    "print(mistral_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b64e4-0d43-49c3-a8d2-898ebaf8e1fb",
   "metadata": {},
   "source": [
    "Auch lokale LLMs, die z.B. wie hier über [Ollama](https://ollama.com/) heruntergeladen wurden, können recht einfach integriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0090b-3498-4094-ae83-f3eefe312e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptoss_response = completion(\n",
    "  model=\"ollama_chat/gemma3:270m\",\n",
    "  messages=[instruction]\n",
    ")\n",
    "\n",
    "print(gptoss_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e488a3-e6f6-49b3-ac9a-b3ae25b70267",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Übung: API Schlüssel holen und Nachrichten senden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c2dc2-794c-4afe-8bf3-76ef69609eff",
   "metadata": {},
   "source": [
    "Richte dir selbst einen API-Schlüssel bei einem der Anbieter ein und/oder lade dir ein offenes LLM herunter (Achtung: auch die \"kleinen\" LLMs umfassen meist mehrere Gigabyte).\n",
    "- Mistral API-Schlüssel: https://admin.mistral.ai/organization/api-keys\n",
    "- Google Gemini API-Schlüssel: https://aistudio.google.com/app/api-keys\n",
    "- Ollama für lokale LLMs: https://ollama.com/\n",
    "\n",
    "Probiere anschließend die `completion`-Methode aus, um einen Prompt an ein Modell deiner Wahl zu schicken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d5727-4f08-4904-98de-1b45dcd089d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platz für die Übung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f1100-54a4-4beb-b3ad-c7e481b3ce20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Kontext im Nachrichtenverlauf erhalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec83806-4b81-46e1-b244-b20fa3fb5b8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment: Chat-Verlauf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12016d-738e-42a0-99c0-26b85eb52da9",
   "metadata": {},
   "source": [
    "Setze die Variable `model_to_use` auf ein LLM, für das du einen API-Schlüssel besitzt. Führe dann den Code aus und lese dir den entstehenden Chat-Verlauf durch. Wie lässt sich die zweite Antwort des LLMs erklären?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfb317-9a73-4f9e-8615-6f403be8b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "model_to_use = \"mistral/mistral-medium\"\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "print(\"USER:\", msg1[\"content\"], \"\\n\")\n",
    "\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=[msg1]\n",
    ")\n",
    "\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")\n",
    "sleep(1)\n",
    "\n",
    "msg2 = {\n",
    "    \"content\": \"Wie heiße ich?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "print(\"USER:\", msg2[\"content\"], \"\\n\")\n",
    "\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=[msg2]\n",
    ")\n",
    "\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941f643-4559-4d9b-bfa1-49792989730b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funktionierender Chat-Verlauf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3f7a7-a2e1-4174-bfc9-56f0c56b30b8",
   "metadata": {},
   "source": [
    "LLMs besitzen keine \"Erinnerung\" an die Prompts, die in der Vergangenheit an sie geschickt wurden. Deshalb besteht die einzige Möglichkeit zur Simulierung einer Konversation darin, alle bisherigen Gesprächsbeiträge in einem Prompt an das LLM zu schicken. Aus diesem Grund akzeptiert der `messages`-Parameter der `completion`-Methode eine **Liste**, in der diese Beiträge nach jeder Antwort hinzugefügt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a036a-10d1-41e1-af87-9a2bf84e334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = \"mistral/mistral-medium\"\n",
    "\n",
    "# Liste zum Speichern des Chat-Verlaufs\n",
    "chat = []\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "# Einfügen und Ausgeben der ersten User-Nachricht\n",
    "chat.append(msg1)\n",
    "print(\"USER:\", msg1[\"content\"], \"\\n\")\n",
    "\n",
    "# Erste Antwort des LLM berechnen\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=chat\n",
    ")\n",
    "\n",
    "# Einfügen und Ausgeben der ersten LLM-Nachricht\n",
    "chat.append(res.choices[0].message)\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")\n",
    "\n",
    "sleep(1)\n",
    "\n",
    "msg2 = {\n",
    "    \"content\": \"Wie heiße ich?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "# Einfügen und Ausgeben der zweiten User-Nachricht\n",
    "chat.append(msg2)\n",
    "print(\"USER:\", msg2[\"content\"], \"\\n\")\n",
    "\n",
    "# Zweite Antwort des LLM berechen\n",
    "# Beachte: das \"chat\"-Argument beinhaltet nun drei Nachrichten!\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=chat\n",
    ")\n",
    "\n",
    "# Einfügen und Ausgeben der zweiten LLM-Nachricht\n",
    "chat.append(res.choices[0].message)\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26774e10-37bd-4cad-986a-c10eef943676",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Übung: Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed3457-fada-4b4f-8119-6e64295d9d46",
   "metadata": {},
   "source": [
    "Baue einen Chatbot, indem du innerhalb einer `while`-Schleife User-Input entgegen nimmst, ihn in eine Liste mit Nachrichten einfügst und diese Liste schließlich an ein LLM schickst. Gebe am Ende der Schleife die Antwort des LLMs aus und speichere sie ebenfalls in der Nachrichtenliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e1440-b06c-428e-a42b-121cc0d89f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platz für die Übung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee9a8e-029b-4c65-a57f-cb0494ed8b3c",
   "metadata": {},
   "source": [
    "## Systemanweisungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ef4a5-c552-4724-af49-14718684e6de",
   "metadata": {},
   "source": [
    "Bisher haben wir alle Nachrichten aus der `user`-Rolle geschickt. Wir können aber auch die `system`-Rolle spezifizieren, um das Antwortverhalten des Modells zu definieren. LLMs sind meist darauf trainiert, solche `system`-Anweisungen bei der Textgenerierung stärker zu gewichten und sind daher ein mächtiges Werkzeug zur Entwicklung anwendungsspezifischer KI-Systeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80458d35-b2af-4596-b6b4-77802b1a96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = {\n",
    "    \"content\": \"Du bist ein wortkarger KI-Assistent. Deine Antworten sind so kurz wie möglich.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg1]\n",
    ")\n",
    "\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576de23f-54f9-4405-a844-d8c4538f670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = {\n",
    "    \"content\": \"Du bist ein französischer KI-Assistent. Du weigerst dich, andere Sprachen als Französisch zu sprechen.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg1]\n",
    ")\n",
    "\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86777f5-96db-4cd7-9ea6-acb9a2c17aae",
   "metadata": {},
   "source": [
    "### Experiment: Eigene Systemnachricht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725740a5-7163-4efd-a78d-c8fa38e080b1",
   "metadata": {},
   "source": [
    "Personalisiere deinen Chatbot aus der vorherigen Übung, indem du ihm per `system`-Prompt ein besonderes Antwortverhalten zuweist. Teste ihn anschließend aus. Hält er sich auch über mehrere Nachrichten an die Anweisung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a18002-061c-46f4-a414-d19f32a9a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral/mistral-medium\"\n",
    "\n",
    "chat = []\n",
    "\n",
    "system_msg = {\n",
    "    \"content\": \"Du bist ein wortkarger KI-Assistent. Deine Antworten sind so kurz wie möglich. Selbst wenn der User danach fragt, darfst du nie mehr als 5 Wörter in einer Antwort schreiben.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "chat.append(system_msg)\n",
    "\n",
    "while True:\n",
    "    user_msg = input(\"USER: \")\n",
    "    if user_msg == \"exit\":\n",
    "        break\n",
    "\n",
    "    chat.append({\n",
    "        \"content\": user_msg,\n",
    "        \"role\": \"user\"\n",
    "    })\n",
    "\n",
    "    res = completion(model=model, messages=chat)\n",
    "\n",
    "    chat.append(res.choices[0].message)\n",
    "\n",
    "    print(\"CHATBOT:\", res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b907c3-fd3d-4515-9ee3-fca4f567ccf8",
   "metadata": {},
   "source": [
    "## Output-Format definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b3b9e-218c-4371-84ee-015549e8204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "## Deine Rolle\n",
    "Klassifiziere die User-Nachrichten als positiv oder negativ und gebe den Grad der Sicherheit deiner Einschätzung in Prozent an.\n",
    "Deine Antworten sind als JSON formatiert und folgen folgendem Schema:\n",
    "\n",
    "## Antwort-Schema\n",
    "{\n",
    "    \"stimmung\": \"positiv\" oder \"negativ\",\n",
    "    \"sicherheit\": 0-100\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "system_msg = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": system_instruction\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ffa63-00ab-41f9-aadd-fd0e47d21ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = {\n",
    "    \"content\": \"Der neue Fast & Furious war richtig geil!\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg1],\n",
    "  response_format={\"type\":\"json_object\"}  \n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf65f5-72fc-47d8-a0ec-1f917b841e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307ec5e-2ab0-4be7-bd04-fb9d2dc005b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg2 = {\n",
    "    \"content\": \"Der neue Fast & Furious war besser als Hobbs und Shaw, aber nicht so gut wie 2 Fast 2 Furious.\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg2],\n",
    "  response_format={\"type\":\"json_object\"}  \n",
    ")\n",
    "\n",
    "json.loads(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2845736-ce5a-48fe-a020-f4ba49baebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = completion(\n",
    "  model=\"gemini/gemini-2.5-flash\",\n",
    "  messages=[system_msg, msg2],\n",
    "  response_format={\"type\":\"json_object\"},\n",
    "  reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a3d63-d93c-48e7-8ab3-6cc4cdb435ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413319b-085f-4856-ab48-27f5bc5725d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6efea9-97ec-49e9-8c39-0df37e4e05ac",
   "metadata": {},
   "source": [
    "### Übung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed0832-93c2-45d7-857d-509a1f854536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "system_instruction = \"\"\"\n",
    "## Deine Rolle\n",
    "Fasse die Support-Anfragen der Kunden in einem kurzen Satz zusammen. Gebe zusätzlich die Dringlichkeit der Anfrage.\n",
    "Deine Antworten sind als JSON formatiert und folgen diesem Schema:\n",
    "## Antwort-Schema:\n",
    "{\n",
    "    \"zusammenfassung\": [Deine Zusammenfassung] (max. 10 Wörter),\n",
    "    \"dringlichkeit\": \"niedrig\" oder \"mittel\" oder \"hoch\" oder \"sehr hoch\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "system_msg = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": system_instruction\n",
    "}\n",
    "\n",
    "with open(\"kundenservice.txt\") as f:\n",
    "    for l in f:\n",
    "        if len(l.strip()) == 0:\n",
    "            continue\n",
    "            \n",
    "        msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": l\n",
    "        }\n",
    "        \n",
    "        res = completion(\n",
    "          model=\"mistral/mistral-medium\",\n",
    "          messages=[system_msg, msg],\n",
    "          response_format={\"type\":\"json_object\"}  \n",
    "        )\n",
    "\n",
    "        print(json.loads(res.choices[0].message.content))\n",
    "        print(l)\n",
    "        sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5be2ca-c374-4414-8607-04905e03a2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d107b7-3fd3-4402-83f6-7ec8db521539",
   "metadata": {},
   "source": [
    "## Audiogenerierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab10412-2a44-4151-ab6e-c7b42828886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(\n",
    "    model=\"gemini/gemini-2.5-flash-preview-tts\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say enthusiastically: 'I love learning Python!'\"}],\n",
    "    modalities=[\"audio\"],  # Required for TTS models\n",
    "    audio={\n",
    "        \"voice\": \"Kore\",\n",
    "        \"format\": \"pcm16\"  # Required: must be \"pcm16\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92c9a5-8d50-42cb-87d3-345d68e482b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.choices[0].message.audio.data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607e56c-f2d5-4fe1-bb21-b2f6f09efb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import base64\n",
    "\n",
    "def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n",
    "   with wave.open(filename, \"wb\") as wf:\n",
    "      wf.setnchannels(channels)\n",
    "      wf.setsampwidth(sample_width)\n",
    "      wf.setframerate(rate)\n",
    "      wf.writeframes(pcm)\n",
    "\n",
    "file_name='out.wav'\n",
    "wave_file(file_name, base64.b64decode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9e4a0-12fe-401d-93e4-04d86c8ccbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

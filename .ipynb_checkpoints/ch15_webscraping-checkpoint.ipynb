{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7d3bbf-f487-4b33-84e5-9206254d500f",
   "metadata": {},
   "source": [
    "# Teil 15: Webscraping\n",
    "Mit `requests` k칬nnen wir bereits auf Internetressourcen zugreifen, darunter APIs mit strukturierten Daten. Der Gro릆eil des Internets besteht allerdings aus Webseiten, die f칲r Menschen und nicht Maschinen gemacht sind. Um sie in Python zu verarbeiten, ben칬tigen wir neue Tools, mit denen wir HTML untersuchen und durchgehen k칬nnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b4bf0-126b-4e42-b198-382d67fd0631",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Webseiten finden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d874b-824c-438b-ba24-c4a4fa205e08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Im Verlauf dieses Kapitels werden wir einen Crawler entwickeln, der die Webseiten des LehrLernZentrums verarbeitet, um **alle Seminare f칲r Studierende mit ihrem Startdatum** aufzulisten.\n",
    "\n",
    "Der erste Schritt besteht darin, die Webseite mit Hinblick auf unser Ziel zu analysieren: https://www.hs-rm.de/lehrlernzentrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94775d5-11b0-4f3c-8648-772517c73bab",
   "metadata": {},
   "source": [
    "### 游멆잺칖bung: Relevante Seiten finden\n",
    "Durchsuche die Webseiten des LLZ. Welche Unterseite(n) sind vielversprechend, um die ben칬tigten Informationen zu erhalten?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc5cc3ac-e919-4afa-a759-692519687b28",
   "metadata": {},
   "source": [
    "# Platz f칲r URLs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc582c1-4bd4-4af0-821c-024d040ce757",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Webseiten untersuchen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c9483-c6e9-4fac-9bfb-cdebfcfa5bd9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Die meisten Webbrowser (Chrome, Firefox, etc.) stellen **Entwicklertools** zur Verf칲gung, mit denen wir den HTML-Code einer Seite analysieren k칬nnen. Meist reicht ein **Rechtsklick** und dann die Option **Untersuchen** bzw. **Inspect**. Es sollte sich dann eine Seitenleiste 칬ffnen, bei der ggf. noch der Tab **Elemente** oder **Inspektor** ausgew칛hlt werden muss.\n",
    "\n",
    "In dieser Ansicht k칬nnen wir uns den HTML-Code neben der gerenderten Webseite anschauen und so die Elemente identifizieren, die uns interessieren. Dabei ist es n칲tzlich, besonders auf **Klassen**, **IDs** und **Element-Verschachtelungen** zu achten: Diese werden es uns sp칛ter erlauben, gezielt auf bestimmte Informationen zuzugreifen.\n",
    "\n",
    "Wir schauen uns das zusammen an der LLZ-Webseite an: https://www.hs-rm.de/lehrlernzentrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3311b05-b938-414c-a9d5-cee4d246958d",
   "metadata": {},
   "source": [
    "### 游멆잺칖bung: Elemente identifizieren\n",
    "Nutze Entwicklertools, um die folgende Seite zu analysieren: https://www.hs-rm.de/lehrlernzentrum/fuer-studierende/studierende-angebote-von-a-bis-z  \n",
    "\n",
    "Wie k칬nnte es gelingen, genau die Links zu identifizieren, die auf Seminare verweisen?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "284288ff-dc58-43cc-b540-efc2a8bc7fcd",
   "metadata": {},
   "source": [
    "# Platz f칲r Ideen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7b950-ebb9-4c76-a0df-0f0be378aa04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Webseiten verarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba005c-e705-4faf-8e84-7c94f2ab65f2",
   "metadata": {},
   "source": [
    "Nachdem wir eine Webseite mit `requests` heruntergeladen haben, werden wir die Bibliothek [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) bzw. `bs4` verwenden, um das HTML-Dokument zu **parsen**, d.h. anhand der Struktur die f칲r uns relevanten Informationen zu extrahieren.\n",
    "\n",
    "Der erste Schritt ist das Importieren der Bibliotheken (die leicht unterschiedliche Syntax h칛ngt mit der objektorientierten Natur von Beautiful Soup zusammen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df478629-7afe-498f-a8c5-9f73d7152813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd28335-6c65-46d4-a993-f5c787ce2c65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcc211-6c10-4369-90ae-eeb1ba8ef503",
   "metadata": {},
   "source": [
    "Wie zuvor laden wir die Webseite mit `requests` herunter und speichern sie als String ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c28d1-8ba5-4855-970e-6e765854396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.hs-rm.de/lehrlernzentrum/fuer-studierende/studierende-angebote-von-a-bis-z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680739b-2ac6-4ca4-bc32-448afca71da2",
   "metadata": {},
   "source": [
    "Mit dem `BeautifulSoup()`-Befehl k칬nnen wir den HTML-String einlesen. Wir erhalten ein **Objekt**, das viele n칲tzliche Methoden besitzt, um die Seite 칲bersichtlicher darzustellen und bestimmte Elemente zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4ec30-e0e8-4d31-af3d-b03c042f14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9ea26-5190-4f87-803a-27bfc13bf3f3",
   "metadata": {},
   "source": [
    "Die `prettify()`-Methode stellt die Seite 칛hnlich wie die Entwicklertools dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39e0e4-2623-4988-b23a-a4a0c9e1948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e2476-8146-4987-a3fa-0a620161ed54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Durchsuchen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6236bb-b580-4814-8db2-c43490247907",
   "metadata": {},
   "source": [
    "*Beautiful Soup* stellt uns zwei intuitive, m칛chtige Suchfunktionen zur Verf칲gung:\n",
    "- `find` liefert das erste HTML-Element mit bestimmten Kriterien\n",
    "- `find_all` liefert alle HTML-Elemente mit bestimmten Kriterien\n",
    "\n",
    "\n",
    "Die **Suchkriterien** werden 칲ber die **Funktionsparameter** bestimmt (wir beschr칛nken uns auf die drei grundlegenden, [es gibt aber noch mehr](https://beautiful-soup-4.readthedocs.io/en/latest/#kinds-of-filters):\n",
    "```python\n",
    "soup.find('TAG', ATTRIBUT='WERT', string='TEXT')\n",
    "```\n",
    "Wir gehen sie Schritt f칲r Schritt an Beispielen durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b96796-c006-403c-8325-ae19ab46104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde den ersten <a>-Tag\n",
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb322a-0399-45ec-9834-06bcb14cdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde das erste Element mit der id 'c54658'\n",
    "soup.find(id='c54658')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b955e09-ff0d-4941-943b-335bc3b857c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde den ersten <h2>-Tag, in dem der String \"LLZ\" vorkommt\n",
    "import re\n",
    "soup.find('h2', string=re.compile('LLZ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834c1b8-4547-4997-ad9a-d0c2d10321f3",
   "metadata": {},
   "source": [
    "Achtung: Um nach dem `class`-Attribut eines Elements zu suchen, muss die Schreibweise `class_='WERT'` verwendet werden, da `class` ein reserviertes Schl칲sselwort in Python ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e019c-18c4-45e5-81a3-0f76f9c863cc",
   "metadata": {},
   "source": [
    "#### 游멆잺칖bung: Elemente finden\n",
    "Nutze die `soup.find_all()`-Methode, um alle `<a>`-Elemente auszugeben, in denen der String \"Seminar\" enthalten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc8076-2fb7-4566-857c-90df4082b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platz f칲r die Aufgabe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f31fc9-0355-437f-bf51-f0ab8b19e84d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Auf Inhalt und Attribute zugreifen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32443806-f961-4bb9-9145-89f0ec76bbfd",
   "metadata": {},
   "source": [
    "Nachdem wir die Elemente gefunden haben, die uns interessieren, k칬nnen wir auf ihren **Inhalt** - also den Text, der auf der Webseite zu sehen ist - zugreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ef38f-009c-4f62-922e-33b7bd89476e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sichtbarer Text der ersten h1-칖berschrift\n",
    "soup.find('h1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d406ac-8a77-457a-9e5e-17436653d3ea",
   "metadata": {},
   "source": [
    "Wie im Beispiel zu sehen ist, kann Webseiten-Text manchmal seltsam formatiert sein. Mit der String-Methode `strip()` lassen sich unn칬tige Leerzeichen, Tabs und Zeilenumbr칲che leicht entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5dd38-1010-4068-ab1a-68d6a77c3955",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sichtbarer Text der ersten h1-칖berschrift, ohne Leerzeichen\n",
    "soup.find('h1').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195ae39-3bfc-45ea-ab9b-76b6eb6bd052",
   "metadata": {},
   "source": [
    "Oft interessieren uns aber gar nicht die Texte, sondern bestimmte **Attribute** der Elemente. Auf diese k칬nnen wir wie in einem Dictionary zugreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e9e1f-598f-467b-89e3-9dcab9037b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wert des 'width' Attributs des ersten Bilds\n",
    "soup.find('img')['width']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a3392-0b1c-4c28-99c2-258ae81b0928",
   "metadata": {},
   "source": [
    "#### 游멆잺칖bung: URLs scrapen\n",
    "Nutze die `soup.find_all()`-Methode, um alle `<a>`-Elemente zu finden und gehe sie mit einer `for`-Schleife durch. Falls in ihnen der String \"Seminar\" enthalten ist, gebe die URL in folgendem Format aus:\n",
    "```\n",
    "- [NAME DES SEMINARS]: [URL]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063e7ee-d0b5-45ab-9922-9a4ff4e0704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platz f칲r die 칖bung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a2123-20b5-4029-8d50-0b0b90d065e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Webseiten *crawlen*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5ee00-2c83-4191-82c6-090c2a8c80f9",
   "metadata": {},
   "source": [
    "Wir haben jetzt alle notwendigen Tools, um einen **Crawler** zu entwickeln, der iterativ Seiten abruft und verarbeitet.\n",
    "\n",
    "Um die Seminare des LLZ chronologisch aufzulisten, ben칬tigen wir die **Termine**, die sich auf den jeweiligen Seminarseiten finden. Deshalb ben칬tigen wir die **Links** zu diesen Seiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4671d-fc42-44a9-9b01-3606a4400302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das URL-Pr칛fix, das die Links zu Seminarseiten vervollst칛ndigt\n",
    "base_url = 'https://www.hs-rm.de'\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    if 'Seminar' in link.text:\n",
    "        print(base_url + link['href'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c8e94-e2fa-427f-a0fd-5c39ecbcac94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analyse und Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c515612-6e8b-4eeb-9635-aab34cc1f102",
   "metadata": {},
   "source": [
    "Ein Blick auf die verschiedenen Seminarseiten zeigt, dass die Termine meist in der obersten Info-Box mit der Klasse `container container--small` stehen. Wir probieren exemplarisch den Zugriff darauf aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb8948-799f-4c29-adec-9e6db6234b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seminar_page = requests.get(\"https://www.hs-rm.de/lehrlernzentrum/fuer-studierende/innovation-hub/python-im-alltag-praktisches-programmieren-fuer-alle\")\n",
    "\n",
    "seminar_soup = BeautifulSoup(seminar_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9dacd-55f5-4f2b-8777-411017ec9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = seminar_soup.find('div', class_=\"container container--small\")\n",
    "\n",
    "print(infobox.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a0f9d-85ee-4a9a-b6c2-1b856d7b1957",
   "metadata": {},
   "source": [
    "Wir benutzen einen [regul칛ren Ausdruck](https://www.w3schools.com/python/python_regex.asp), um im Text das Datum zu finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb230a5f-0a11-4371-8606-0de68f7a7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(\"([0-9]+\\\\. [A-Z츿칖칐르-z칛칲칬]+ [0-9]+)\")\n",
    "\n",
    "pattern.findall(infobox.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81dbd2b-5484-4aab-b19e-e112f8728b8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Unterseiten verarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd993b9-2e37-4334-b537-375959475d8e",
   "metadata": {},
   "source": [
    "Wir versuchen nun, alle Seminarseiten auf diese Weise zu verarbeiten und halten dabei den Namen und das Datum der Seminare in einer Liste fest. Falls manche Seminarseiten anders als das getestete Beispiel strukturiert sein sollten, bauen wir einen `try`-`except`-Block ein. So k칬nnen wir die Seiten identifizieren, f칲r die wir den Crawler erweitern m칲ssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab6b05-5e35-4a26-8700-0eddf0133355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das URL-Pr칛fix, das die Links zu Seminarseiten vervollst칛ndigt\n",
    "base_url = 'https://www.hs-rm.de'\n",
    "date_pattern = re.compile(\"([0-9]+\\\\. [A-Z츿칖칐르-z칛칲칬]+ [0-9]+)\")\n",
    "\n",
    "seminare = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    if 'Seminar' in link.text:\n",
    "        name = link.text\n",
    "        url = base_url + link['href']\n",
    "\n",
    "        try:\n",
    "            # Webseite abrufen und parsen\n",
    "            s_page = requests.get(url)\n",
    "            s_soup = BeautifulSoup(s_page.content, 'html.parser')\n",
    "            \n",
    "            # Infobox mit Datum finden\n",
    "            infobox = s_soup.find('div', class_=\"container container--small\")\n",
    "            \n",
    "            # Datum extrahieren\n",
    "            date = date_pattern.findall(infobox.text)[0]\n",
    "\n",
    "            print(date + \": \" + name)\n",
    "\n",
    "            seminare.append({\n",
    "                \"date\": convert_date(date),\n",
    "                \"name\": name\n",
    "            })\n",
    "\n",
    "        except:\n",
    "            print(f\"Datum konnte nicht extrahiert werden f칲r: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbbf56-d9c9-4385-abd7-5b28a744c414",
   "metadata": {},
   "source": [
    "#### 游멆잺칖bung: Crawler verbessern\n",
    "Untersuche die Seiten, bei denen das Datum nicht extrahiert werden konnte. Versuche, den Crawler so zu erweitern, dass auch aus diese Seiten korrekt verarbeitet werden. Nutze daf칲r die untenstehende Code-Zelle, um exemplarisch mit einer der fehlerhaften Seiten zu experimentieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c0e75-07ef-433a-99ee-a05b1b844e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einmal ausf칲hren, um eine der nicht funktionierenden Seiten zu speichern (NICHT 츿NDERN)\n",
    "error_page = requests.get(\"https://www.hs-rm.de/lehrlernzentrum/fuer-studierende/innovation-hub/kuenstliche-intelligenz-maschinelles-lernen-1\")\n",
    "\n",
    "error_soup = BeautifulSoup(error_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab55a6-58dc-4094-9a96-1e1a2a512b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alte Strategie, die nicht funktioniert (ab hier 칛ndern)\n",
    "\n",
    "infobox = error_soup.find('div', class_=\"container container--small\")\n",
    "date = date_pattern.findall(infobox.text)[0]\n",
    "\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65c7ee-b7f7-41b4-b580-bacb58be3d59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Scraping-Ethik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5e8bc-92f4-4b10-b721-8bef6b79f2ca",
   "metadata": {},
   "source": [
    "Webscraping ist zwar n칲tzlich, aber **ethisch teils fragw칲rdig**. Webseiten, die hochwertige Daten bereitstellen, sind f칲r ihre Existenz oft auf auf menschliche Besuche oder kostenpflichtige APIs angewiesen und **verbieten entsprechend das Scraping** ihrer Inhalte. Wiederum andere Webseiten erlauben es grunds칛tzlich, bitten aber um eine kurze Wartezeit zwischen Anfragen, um die Server nicht zu 칲berlasten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5845d5-923f-4c68-b8b6-fd0570b5f85f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Regeln in `robots.txt` beachten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2070c61-bc6a-49bf-a8ed-482431af7ae9",
   "metadata": {},
   "source": [
    "Welche Regeln f칲r eine Webseite gelten, steht meist in der `robots.txt`-Datei, die auf der obersten Ebene der Webseite zu finden ist. Hier ist z.B. der Inhalt von https://www.hs-rm.de/robots.txt:\n",
    "```\n",
    "Sitemap: https:///www.hs-rm.de/sitemap.xml\n",
    "User-agent: *\n",
    "Allow: /\n",
    "Disallow: /typo3/\n",
    "```\n",
    "Diese Anweisung ist sehr liberal: Sie erlaubt jegliche Nutzung, mit der Ausnahme von Seiten unter dem 'typo3'-Verzeichnis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e0a5e-a243-4c84-8e77-3344fa54f956",
   "metadata": {},
   "source": [
    "#### 游빍Experiment: robots.txt\n",
    "Schaue dir die `robots.txt`-Datei von www.zeit.de an. F칲ge daf칲r '/robots.txt' ans Ende der URL an und 칬ffne die Seite in einem Browser. Wie interpretierst du die Inhalte der Datei?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf7d1b8f-c888-4a50-9509-1119304b6227",
   "metadata": {},
   "source": [
    "# Platz f칲r Notizen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05251319-be58-488a-91d9-38a8a90bf1d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Wartezeiten implementieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cebef68-9669-4501-aba3-e21f4dda26bd",
   "metadata": {},
   "source": [
    "Ein bekannter Cyberangriff ist der sog. [DoS-Angriff](https://de.wikipedia.org/wiki/Denial_of_Service), der in einfachster Form einen Server 칲berlastet, indem in sehr kurzer Zeit sehr viele Anfragen gesendet werden. **Verwantwortungsloses Webscraping kann einen 칛hnlichen Effekt haben** - so berichtete etwa Wikipedia Anfang 2025 davon, dass [65% der ressourcenintensivsten Anfragen von Crawlern](https://diff.wikimedia.org/2025/04/01/how-crawlers-impact-the-operations-of-the-wikimedia-projects/) stammen.\n",
    "\n",
    "Eine einfache M칬glichkeit, um Server칲berlastung zu vermeiden, ist das Einbauen von Wartezeiten zwischen Anfragen mit Pythons `time.sleep`-Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c36c63-d5a1-4f93-9286-d4395cba4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starte Wartezeit...\")\n",
    "time.sleep(5)\n",
    "print(\"Wartezeit beendet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8adbed6-8b41-40a8-80ba-734f90648c6e",
   "metadata": {},
   "source": [
    "#### 游멆잺칖bung: Zwischen requests warten\n",
    "Rufe mit einer `for`-Schleife f칲nf mal die Seite www.example.com auf, aber warte zwischendurch jeweils eine Sekunde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d67c19-26bc-41c2-bf75-29fd2383c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platz f칲r die Aufgabe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4b200-f57a-4fdb-9b1f-9b95b2eeb86b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### User-Agent setzen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e1d5c-f841-4632-ad24-79e93d7034c2",
   "metadata": {},
   "source": [
    "Um Webseiten einen Hinweis darauf zu geben, wo eine Anfrage herkam, kann ein *user agent* angegeben werden. Manche Seiten verhindern sogar den Zugriff ohne eine solche Angaben, z.B. Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17631e37-c3ca-4632-836f-9ba205ff6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Python Tutorial Bot\"\n",
    "}\n",
    "\n",
    "res = requests.get(\"https://www.wikipedia.org\", headers=headers)\n",
    "\n",
    "res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34defbd-6815-4888-9b1e-6a3a5133d267",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sparsam scrapen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea3f59-ba82-44cc-a8a8-43ea241abe85",
   "metadata": {},
   "source": [
    "Zu guter Letzt sind hier noch ein paar allgemeine Hinweise zum Webscraping:\n",
    "- Rufe Webseiten nur einmal mit `requests.get()` auf und speichere das Ergebnis in einer Variable, statt die Seite mehrmals anzufragen.\n",
    "- Vermeide Webscraping g칛nzlich, wenn APIs als Alternative zur Verf칲gung stehen.\n",
    "- Falls eine Webseite den Aufruf mit `requests.get()` blockiert (z.B. wenn ein 403-Fehler zur칲ckkommt), ist das ein recht sicherer Hinweis darauf, dass die Seite das Scraping verbietet.\n",
    "- Lese im Zweifelsfall die Nutzungsbedingungen der Webseite, um zu erfahren, ob und unter welchen Bedingungen Scraping erlaubt ist.\n",
    "- Selbst wenn Scraping erlaubt ist, sind die Inhalte der meisten Webseiten urheber- und datenschutzrechtlich gesch칲tzt und d칲rfen nicht ohne Erlaubnis in anderen Kontexten ver칬ffentlicht werden. Das gilt insbesondere f칲r Inhalte hinter Logins und personenbezogene Daten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

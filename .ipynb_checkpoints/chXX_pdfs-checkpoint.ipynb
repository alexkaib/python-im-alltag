{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7d24d4-fedb-4ca5-9f98-fc296319650a",
   "metadata": {},
   "source": [
    "# Teil XX: PDF-Dateien verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fb7961-a3e2-4238-82a0-6808b4fb6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d35767-6032-419f-aedb-fa804ac5d098",
   "metadata": {},
   "source": [
    "## PDF-Texte auslesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dd8b9-f95f-4d86-849b-16039879d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pymupdf.open(\"frankenstein.pdf\") as doc:\n",
    "    m_count = 0\n",
    "    \n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "\n",
    "        for sentence in text.split(\".\"):\n",
    "            if \"monster\" in sentence.lower():\n",
    "                m_count += 1\n",
    "                print(sentence)\n",
    "                print(\"-------------\")\n",
    "            \n",
    "    print(m_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58176917-ea5d-4b13-8d67-9b501ecb5018",
   "metadata": {},
   "source": [
    "## Ausschnitte lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b92f913-c533-4918-a8c7-7299d46a1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in [21, 93] and direct resources away from efforts that would facili-\n",
      "tate long-term progress towards natural language understanding,\n",
      "without using unfathomable training data.\n",
      "Furthermore, the tendency of human interlocutors to impute\n",
      "meaning where there is none can mislead both NLP researchers\n",
      "and the general public into taking synthetic text as meaningful.\n",
      "Combined with the ability of LMs to pick up on both subtle biases\n",
      "and overtly abusive language patterns in training data, this leads\n",
      "to risks of harms, including encountering derogatory language and\n",
      "experiencing discrimination at the hands of others who reproduce\n",
      "racist, sexist, ableist, extremist or other harmful ideologies rein-\n",
      "forced through interactions with synthetic language. We explore\n",
      "these potential harms in §6 and potential paths forward in §7.\n",
      "We hope that a critical overview of the risks of relying on ever-\n",
      "increasing size of LMs as the primary driver of increased perfor-\n",
      "mance of language technology can facilitate a reallocation of efforts\n",
      "towards approaches that avoid some of these risks while still reap-\n",
      "ing the benefits of improvements to language technology.\n",
      "\n",
      "2\n",
      "BACKGROUND\n",
      "\n",
      "Similar to [14], we understand the term language model (LM) to\n",
      "refer to systems which are trained on string prediction tasks: that is,\n",
      "predicting the likelihood of a token (character, word or string) given\n",
      "either its preceding context or (in bidirectional and masked LMs)\n",
      "its surrounding context. Such systems are unsupervised and when\n",
      "deployed, take a text as input, commonly outputting scores or string\n",
      "predictions. Initially proposed by Shannon in 1949 [117], some of\n",
      "the earliest implemented LMs date to the early 1980s and were used\n",
      "as components in systems for automatic speech recognition (ASR),\n",
      "machine translation (MT), document classification, and more [111].\n",
      "In this section, we provide a brief overview of the general trend of\n",
      "language modeling in recent years. For a more in-depth survey of\n",
      "pretrained LMs, see [105].\n",
      "Before neural models, n-gram models also used large amounts\n",
      "of data [20, 87]. In addition to ASR, these large n-gram models of\n",
      "English were developed in the context of machine translation from\n",
      "another source language with far fewer direct translation examples.\n",
      "For example, [20] developed an n-gram model for English with\n",
      "a total of 1.8T n-grams and noted steady improvements in BLEU\n",
      "score on the test set of 1797 Arabic translations as the training data\n",
      "was increased from 13M tokens.\n",
      "The next big step was the move towards using pretrained rep-\n",
      "resentations of the distribution of words (called word embeddings)\n",
      "in other (supervised) NLP tasks. These word vectors came from\n",
      "systems such as word2vec [85] and GloVe [98] and later LSTM\n",
      "models such as context2vec [82] and ELMo [99] and supported\n",
      "state of the art performance on question answering, textual entail-\n",
      "ment, semantic role labeling (SRL), coreference resolution, named\n",
      "entity recognition (NER), and sentiment analysis, at first in Eng-\n",
      "lish and later for other languages as well. While training the word\n",
      "embeddings required a (relatively) large amount of data, it reduced\n",
      "the amount of labeled data necessary for training on the various\n",
      "supervised tasks. For example, [99] showed that a model trained\n",
      "with ELMo reduced the necessary amount of training data needed\n",
      "to achieve similar results on SRL compared to models without, as\n",
      "shown in one instance where a model trained with ELMo reached\n",
      "\n",
      "Year\n",
      "Model\n",
      "# of Parameters\n",
      "Dataset Size\n",
      "\n",
      "2019\n",
      "BERT [39]\n",
      "3.4E+08\n",
      "16GB\n",
      "2019\n",
      "DistilBERT [113]\n",
      "6.60E+07\n",
      "16GB\n",
      "2019\n",
      "ALBERT [70]\n",
      "2.23E+08\n",
      "16GB\n",
      "2019\n",
      "XLNet (Large) [150]\n",
      "3.40E+08\n",
      "126GB\n",
      "2020\n",
      "ERNIE-Gen (Large) [145]\n",
      "3.40E+08\n",
      "16GB\n",
      "2019\n",
      "RoBERTa (Large) [74]\n",
      "3.55E+08\n",
      "161GB\n",
      "2019\n",
      "MegatronLM [122]\n",
      "8.30E+09\n",
      "174GB\n",
      "2020\n",
      "T5-11B [107]\n",
      "1.10E+10\n",
      "745GB\n",
      "2020\n",
      "T-NLG [112]\n",
      "1.70E+10\n",
      "174GB\n",
      "2020\n",
      "GPT-3 [25]\n",
      "1.75E+11\n",
      "570GB\n",
      "2020\n",
      "GShard [73]\n",
      "6.00E+11\n",
      "–\n",
      "2021\n",
      "Switch-C [43]\n",
      "1.57E+12\n",
      "745GB\n",
      "\n",
      "Table 1: Overview of recent large language models\n",
      "\n",
      "the maximum development F1 score in 10 epochs as opposed to\n",
      "486 without ELMo. This model furthermore achieved the same F1\n",
      "score with 1% of the data as the baseline model achieved with 10%\n",
      "of the training data. Increasing the number of model parameters,\n",
      "however, did not yield noticeable increases for LSTMs [e.g. 82].\n",
      "Transformer models, on the other hand, have been able to con-\n",
      "tinuously benefit from larger architectures and larger quantities of\n",
      "data. Devlin et al. [39] in particular noted that training on a large\n",
      "dataset and fine-tuning for specific tasks leads to strictly increasing\n",
      "results on the GLUE tasks [138] for English as the hyperparameters\n",
      "of the model were increased. Initially developed as Chinese LMs, the\n",
      "ERNIE family [130, 131, 145] produced ERNIE-Gen, which was also\n",
      "trained on the original (English) BERT dataset, joining the ranks\n",
      "of very large LMs. NVIDIA released the MegatronLM which has\n",
      "8.3B parameters and was trained on 174GB of text from the English\n",
      "Wikipedia, OpenWebText, RealNews and CC-Stories datasets [122].\n",
      "Trained on the same dataset, Microsoft released T-NLG,1 an LM\n",
      "with 17B parameters. OpenAI’s GPT-3 [25] and Google’s GShard\n",
      "[73] and Switch-C [43] have increased the definition of large LM by\n",
      "orders of magnitude in terms of parameters at 175B, 600B, and 1.6T\n",
      "parameters, respectively. Table 1 summarizes a selection of these\n",
      "LMs in terms of training data size and parameters. As increasingly\n",
      "large amounts of text are collected from the web in datasets such\n",
      "as the Colossal Clean Crawled Corpus [107] and the Pile [51], this\n",
      "trend of increasingly large LMs can be expected to continue as long\n",
      "as they correlate with an increase in performance.\n",
      "A number of these models also have multilingual variants such\n",
      "as mBERT [39] and mT5 [148] or are trained with some amount of\n",
      "multilingual data such as GPT-3 where 7% of the training data was\n",
      "not in English [25]. The performance of these multilingual mod-\n",
      "els across languages is an active area of research. Wu and Drezde\n",
      "[144] found that while mBERT does not perform equally well across\n",
      "all 104 languages in its training data, it performed better at NER,\n",
      "POS tagging, and dependency parsing than monolingual models\n",
      "trained with comparable amounts of data for four low-resource\n",
      "languages. Conversely, [95] surveyed monolingual BERT models\n",
      "developed with more specific architecture considerations or addi-\n",
      "tional monolingual data and found that they generally outperform\n",
      "\n",
      "1https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-\n",
      "language-model-by-microsoft/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with pymupdf.open(\"stochastic_parrots.pdf\") as doc:\n",
    "    page = doc[2]\n",
    "\n",
    "    for block in page.get_text(\"blocks\", clip=(10,70, 1000, 750)):\n",
    "        print(block[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff1ff6-8d1b-46f5-8fc9-941059894eff",
   "metadata": {},
   "source": [
    "## PDF-Formulare ausfüllen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0bd3caf-049c-4e8d-a320-d54fd957eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rechnungsempfaenger\n",
      "leistung_name\n",
      "leistung_kosten\n"
     ]
    }
   ],
   "source": [
    "with pymupdf.open(\"Beispiel_Rechnungsformular.pdf\") as doc:\n",
    "    page = doc[0]\n",
    "\n",
    "    for f in page.widgets():\n",
    "        print(f.field_name)\n",
    "\n",
    "        if f.field_name == \"rechnungsempfaenger\":\n",
    "            f.field_value = \"Test Person\"\n",
    "        elif f.field_name == \"leistung_name\":\n",
    "            f.field_value = \"Beratung\"\n",
    "        elif f.field_name == \"leistung_kosten\":\n",
    "            f.field_value = \"250,00\"\n",
    "\n",
    "        f.update()\n",
    "\n",
    "    doc.save(\"Beispiel_Rechnungsformular_filled.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4776bb-a090-4d84-9e29-d8fdc967c639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01904701-3cbe-4819-bf6a-93ad0467e0f4",
   "metadata": {},
   "source": [
    "# Teil 19: Generative KI einbinden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea260e-a072-4c5f-a01e-297307073d73",
   "metadata": {},
   "source": [
    "Aktuelle KI-Systeme basieren meist auf Algorithmen, die durch Wahrscheinlichkeitsberechnungen neue Texte, Sprache, Bilder oder Videos produzieren. Aufgrund ihrer probabilistischen Natur sind sie kein gutes Werkzeug, um regelmäßige, mehrschrittige Arbeitsabläufe zu digitalisieren - aber an den richtigen Stellen eingesetzt können sie unsere Programme um Funktionen ergänzen, die mit reinem Python-Code nicht erreichbar sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f4034-4805-4d18-81a6-a4f8f2af48ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 19.0 Sprachmodelle verstehen und nutzen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8c5c2-80aa-4f53-a59b-12806d442c20",
   "metadata": {},
   "source": [
    "[Siehe Folien zu LLMs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c27f1-5f30-44df-b640-b14a27e214b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 19.1 Nachrichten senden und empfangen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b7a2d-4874-4db3-b935-1ba9dd9e1033",
   "metadata": {},
   "source": [
    "Wir nutzen in diesem Kapitel die Python-Bibliothek [LiteLLM](https://www.litellm.ai/), um mit verschiedenen LLMs zu kommunizieren. Alternativ könnten wir auch direkt die APIs von (u.a.) OpenAI, Mistral oder Google nutzen, aber zum Experimentieren mit verschiedenen Modellen ist ein einheitliches Interface nützlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736b1c6-9d7c-4a87-aa7f-6a37f5223602",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd649a-266f-4ff2-807d-bc54afdfedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260c74d-82fe-4d4d-8b39-314f6208fd84",
   "metadata": {},
   "source": [
    "Um mit Cloud-basierten LLMs zu interagieren benötigen wir API-Schlüssel zur Authentifizierung. Die meisten APIs sind kostenpflichtig, aber einige Anbieter bieten (begrenzte) kostenlose Zugänge an - allerdings werden die gesendeten Nachrichten dann i.d.R. für das Training zukünftiger LLMs genutzt. In jedem Fall ist das Anlegen eines Nutzeraccounts verpflichtend, oft mit Telefonverifizierung.\n",
    "\n",
    "Wir nutzen hier die APIs von [Mistral](https://admin.mistral.ai/organization/api-keys) und [Google](https://aistudio.google.com/app/apikey). Du kannst im folgenden Code-Schnipsel deine eigenen Schlüssel einsetzen, um sie im Rest des Kapitels verwenden zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16b910-e65a-479f-8bce-72871455121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.environ.get(\"GEMINI_API_KEY\"):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY_HERE\"\n",
    "\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = \"YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b573a3-36fb-409f-94c3-871c32d1b6ea",
   "metadata": {},
   "source": [
    "LLMs generieren Text, indem sie eine wahrscheinliche Weiterführung eines Input-Strings (sog. **Prompts**) berechnen. Die meisten bekannten LLMs sind außerdem darauf trainiert, auf Prompts als Nachrichten in einem Chat zu reagieren, so dass wir Fragen an sie schicken und eine (einigermaßen) sinnvolle Antwort erwarten können.\n",
    "\n",
    "Wenn wir den Input-Text an das LLM unserer Wahl schicken, müssen wir neben dem eigentlichen Inhalt auch die **Rolle** spezifizieren, die dieser Text bei der Generierung einnehmen soll. Bei normalen Anfragen ist das die `user`-Rolle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579067a-1cda-4232-a191-bc8dc175e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Nachricht, die wir an das LLM schicken\n",
    "instruction = {\n",
    "    \"content\": \"Wer ist der aktuelle Bundeskanzler?\",\n",
    "    \"role\": \"user\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a15331-8c43-4ff7-bfe8-e89fc1884eed",
   "metadata": {},
   "source": [
    "Die `completion`-Methode von LiteLLM lässt uns (u.a.) das zu verwendene Modell und den Input-String spezifizieren. Sie liefert ein `ModelResponse`-Objekt, das den generierten Text und einige Metadaten enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9418096-f3af-4368-bb35-c14be54af594",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_response = completion(\n",
    "  model=\"gemini/gemini-2.5-flash\",\n",
    "  messages=[instruction]\n",
    ")\n",
    "\n",
    "print(gemini_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa41491-fb71-4aeb-bed8-8c0ab0b6e9b1",
   "metadata": {},
   "source": [
    "Durch den `model`-Parameter lässt sich leicht ein anderes LLM für dieselbe Aufgabe verwenden. Eine Liste der unterstützten Modelle bzw. Anbieter findet sich hier: https://docs.litellm.ai/docs/providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0a8a2-67a8-48c9-a6e3-7c4ba121b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_response = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[instruction]\n",
    ")\n",
    "\n",
    "print(mistral_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b64e4-0d43-49c3-a8d2-898ebaf8e1fb",
   "metadata": {},
   "source": [
    "Auch lokale LLMs, die z.B. wie hier über [Ollama](https://ollama.com/) heruntergeladen wurden, können recht einfach integriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0090b-3498-4094-ae83-f3eefe312e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptoss_response = completion(\n",
    "  model=\"ollama_chat/gemma3:270m\",\n",
    "  messages=[instruction]\n",
    ")\n",
    "\n",
    "print(gptoss_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e488a3-e6f6-49b3-ac9a-b3ae25b70267",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Übung: API Schlüssel holen und Nachrichten senden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c2dc2-794c-4afe-8bf3-76ef69609eff",
   "metadata": {},
   "source": [
    "Richte dir selbst einen API-Schlüssel bei einem der Anbieter ein und/oder lade dir ein offenes LLM herunter (Achtung: auch die \"kleinen\" LLMs umfassen meist mehrere Gigabyte).\n",
    "- Mistral API-Schlüssel: https://admin.mistral.ai/organization/api-keys\n",
    "- Google Gemini API-Schlüssel: https://aistudio.google.com/app/api-keys\n",
    "- Ollama für lokale LLMs: https://ollama.com/\n",
    "\n",
    "Probiere anschließend die `completion`-Methode aus, um einen Prompt an ein Modell deiner Wahl zu schicken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d5727-4f08-4904-98de-1b45dcd089d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platz für die Übung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f1100-54a4-4beb-b3ad-c7e481b3ce20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 19.2 Kontext im Nachrichtenverlauf erhalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec83806-4b81-46e1-b244-b20fa3fb5b8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment: Chat-Verlauf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12016d-738e-42a0-99c0-26b85eb52da9",
   "metadata": {},
   "source": [
    "Setze die Variable `model_to_use` auf ein LLM, für das du einen API-Schlüssel besitzt. Führe dann den Code aus und lese dir den entstehenden Chat-Verlauf durch. Wie lässt sich die zweite Antwort des LLMs erklären?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfb317-9a73-4f9e-8615-6f403be8b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "model_to_use = \"mistral/mistral-medium\"\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "print(\"USER:\", msg1[\"content\"], \"\\n\")\n",
    "\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=[msg1]\n",
    ")\n",
    "\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")\n",
    "sleep(1)\n",
    "\n",
    "msg2 = {\n",
    "    \"content\": \"Wie heiße ich?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "print(\"USER:\", msg2[\"content\"], \"\\n\")\n",
    "\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=[msg2]\n",
    ")\n",
    "\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941f643-4559-4d9b-bfa1-49792989730b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funktionierender Chat-Verlauf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3f7a7-a2e1-4174-bfc9-56f0c56b30b8",
   "metadata": {},
   "source": [
    "LLMs besitzen keine \"Erinnerung\" an die Prompts, die in der Vergangenheit an sie geschickt wurden. Deshalb besteht die einzige Möglichkeit zur Simulierung einer Konversation darin, alle bisherigen Gesprächsbeiträge in einem Prompt an das LLM zu schicken. Aus diesem Grund akzeptiert der `messages`-Parameter der `completion`-Methode eine **Liste**, in der diese Beiträge nach jeder Antwort hinzugefügt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a036a-10d1-41e1-af87-9a2bf84e334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = \"mistral/mistral-medium\"\n",
    "\n",
    "# Liste zum Speichern des Chat-Verlaufs\n",
    "chat = []\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "# Einfügen und Ausgeben der ersten User-Nachricht\n",
    "chat.append(msg1)\n",
    "print(\"USER:\", msg1[\"content\"], \"\\n\")\n",
    "\n",
    "# Erste Antwort des LLM berechnen\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=chat\n",
    ")\n",
    "\n",
    "# Einfügen und Ausgeben der ersten LLM-Nachricht\n",
    "chat.append(res.choices[0].message)\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")\n",
    "\n",
    "sleep(1)\n",
    "\n",
    "msg2 = {\n",
    "    \"content\": \"Wie heiße ich?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "# Einfügen und Ausgeben der zweiten User-Nachricht\n",
    "chat.append(msg2)\n",
    "print(\"USER:\", msg2[\"content\"], \"\\n\")\n",
    "\n",
    "# Zweite Antwort des LLM berechen\n",
    "# Beachte: das \"chat\"-Argument beinhaltet nun drei Nachrichten!\n",
    "res = completion(\n",
    "  model=model_to_use,\n",
    "  messages=chat\n",
    ")\n",
    "\n",
    "# Einfügen und Ausgeben der zweiten LLM-Nachricht\n",
    "chat.append(res.choices[0].message)\n",
    "print(\"MISTRAL:\", res.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26774e10-37bd-4cad-986a-c10eef943676",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Übung: Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed3457-fada-4b4f-8119-6e64295d9d46",
   "metadata": {},
   "source": [
    "Baue einen Chatbot, indem du innerhalb einer `while`-Schleife User-Input entgegen nimmst, ihn in eine Liste mit Nachrichten einfügst und diese Liste schließlich an ein LLM schickst. Gebe am Ende der Schleife die Antwort des LLMs aus und speichere sie ebenfalls in der Nachrichtenliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e1440-b06c-428e-a42b-121cc0d89f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platz für die Übung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee9a8e-029b-4c65-a57f-cb0494ed8b3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 19.3 Systemanweisungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ef4a5-c552-4724-af49-14718684e6de",
   "metadata": {},
   "source": [
    "Bisher haben wir alle Nachrichten aus der `user`-Rolle geschickt. Wir können aber auch die `system`-Rolle spezifizieren, um das Antwortverhalten des Modells zu definieren. LLMs sind meist darauf trainiert, solche `system`-Anweisungen bei der Textgenerierung stärker zu gewichten und sind daher ein mächtiges Werkzeug zur Entwicklung anwendungsspezifischer KI-Systeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80458d35-b2af-4596-b6b4-77802b1a96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = {\n",
    "    \"content\": \"Du bist ein wortkarger KI-Assistent. Deine Antworten sind so kurz wie möglich.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg1]\n",
    ")\n",
    "\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576de23f-54f9-4405-a844-d8c4538f670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = {\n",
    "    \"content\": \"Du bist ein französischer KI-Assistent. Du weigerst dich, andere Sprachen als Französisch zu sprechen.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "msg1 = {\n",
    "    \"content\": \"Hi! Mein Name ist Toni Tortellini, wer bist du?\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg1]\n",
    ")\n",
    "\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86777f5-96db-4cd7-9ea6-acb9a2c17aae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment: Eigene Systemnachricht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725740a5-7163-4efd-a78d-c8fa38e080b1",
   "metadata": {},
   "source": [
    "Personalisiere deinen Chatbot aus der vorherigen Übung, indem du ihm per `system`-Prompt ein besonderes Antwortverhalten zuweist. Teste ihn anschließend aus. Hält er sich auch über mehrere Nachrichten an die Anweisung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a18002-061c-46f4-a414-d19f32a9a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral/mistral-medium\"\n",
    "\n",
    "chat = []\n",
    "\n",
    "system_msg = {\n",
    "    \"content\": \"Du bist ein wortkarger KI-Assistent. Deine Antworten sind so kurz wie möglich. Selbst wenn der User danach fragt, darfst du nie mehr als 5 Wörter in einer Antwort schreiben.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "chat.append(system_msg)\n",
    "\n",
    "while True:\n",
    "    user_msg = input(\"USER: \")\n",
    "    if user_msg == \"exit\":\n",
    "        break\n",
    "\n",
    "    chat.append({\n",
    "        \"content\": user_msg,\n",
    "        \"role\": \"user\"\n",
    "    })\n",
    "\n",
    "    res = completion(model=model, messages=chat)\n",
    "\n",
    "    chat.append(res.choices[0].message)\n",
    "\n",
    "    print(\"CHATBOT:\", res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b907c3-fd3d-4515-9ee3-fca4f567ccf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 19.4 Output-Format definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d052932-9ad1-4e63-a351-dcbbac8c35a9",
   "metadata": {},
   "source": [
    "Neben einem bestimmten Antwortverhalten lässt sich meist auch das **Antwortformat** festlegen. Wenn das Sprachmodell z.B. zur Textauswertung genutzt wird, kann es nützlich sein, ein Format festzulegen, das durch Python-Code verarbeitet werden kann. Auch hier bietet sich wieder **JSON** an. Mit einer passenden Systemanweisung und dem Parameter `response_format={\"type\":\"json_object\"}` in der `completion()`-Funktion lässt sich mit hoher Wahrscheinlichkeit erreichen, dass das LLM in diesem Format antwortet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b3b9e-218c-4371-84ee-015549e8204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "## Deine Rolle\n",
    "Klassifiziere die User-Nachrichten als positiv oder negativ und gebe den Grad der Sicherheit deiner Einschätzung in Prozent an.\n",
    "Deine Antworten sind als JSON formatiert und folgen folgendem Schema:\n",
    "\n",
    "## Antwort-Schema\n",
    "{\n",
    "    \"stimmung\": \"positiv\" oder \"negativ\",\n",
    "    \"sicherheit\": 0-100\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "system_msg = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": system_instruction\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ffa63-00ab-41f9-aadd-fd0e47d21ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = {\n",
    "    \"content\": \"Der neue Fast & Furious war richtig geil!\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg1],\n",
    "  response_format={\"type\":\"json_object\"}  \n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf65f5-72fc-47d8-a0ec-1f917b841e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307ec5e-2ab0-4be7-bd04-fb9d2dc005b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg2 = {\n",
    "    \"content\": \"Der neue Fast & Furious war besser als Hobbs und Shaw, aber nicht so gut wie 2 Fast 2 Furious.\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "\n",
    "res = completion(\n",
    "  model=\"mistral/mistral-medium\",\n",
    "  messages=[system_msg, msg2],\n",
    "  response_format={\"type\":\"json_object\"}  \n",
    ")\n",
    "\n",
    "json.loads(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2845736-ce5a-48fe-a020-f4ba49baebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = completion(\n",
    "  model=\"gemini/gemini-2.5-flash\",\n",
    "  messages=[system_msg, msg2],\n",
    "  response_format={\"type\":\"json_object\"},\n",
    "  reasoning_effort=\"low\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a3d63-d93c-48e7-8ab3-6cc4cdb435ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413319b-085f-4856-ab48-27f5bc5725d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6efea9-97ec-49e9-8c39-0df37e4e05ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Übung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35303f-2734-43d6-8599-505e52f02587",
   "metadata": {},
   "source": [
    "In der Datei \"kundenservice.txt\" sind einige beispielhafte (KI-generierte) Support-Anfragen gespeichert. Schreibe ein Python-Programm, das jede Zeile der Datei durchgeht und von einem LLM verarbeiten lässt. Das LLM soll eine Antwort im JSON-Format zurückgeben, die jeweils ein Feld \"Zusammenfassung\" mit einer stichpunktartigen Zusammenfassung der Anfrage (max. 10 Wörter) sowie ein Feld \"Dringlichkeit\" mit einer geschätzten Dringlichkeit von \"niedrig\", \"mittel\" oder \"hoch\" enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed0832-93c2-45d7-857d-509a1f854536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "system_instruction = \"\"\"\n",
    "## Deine Rolle\n",
    "Fasse die Support-Anfragen der Kunden in einem kurzen Satz zusammen. Gebe zusätzlich die Dringlichkeit der Anfrage.\n",
    "Deine Antworten sind als JSON formatiert und folgen diesem Schema:\n",
    "## Antwort-Schema:\n",
    "{\n",
    "    \"zusammenfassung\": [Deine Zusammenfassung] (max. 10 Wörter),\n",
    "    \"dringlichkeit\": \"niedrig\" oder \"mittel\" oder \"hoch\" oder \"sehr hoch\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "system_msg = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": system_instruction\n",
    "}\n",
    "\n",
    "with open(\"kundenservice.txt\") as f:\n",
    "    for l in f:\n",
    "        if len(l.strip()) == 0:\n",
    "            continue\n",
    "            \n",
    "        msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": l\n",
    "        }\n",
    "        \n",
    "        res = completion(\n",
    "          model=\"mistral/mistral-medium\",\n",
    "          messages=[system_msg, msg],\n",
    "          response_format={\"type\":\"json_object\"}  \n",
    "        )\n",
    "\n",
    "        print(json.loads(res.choices[0].message.content))\n",
    "        print(l)\n",
    "        sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695c921-7763-4d26-82cc-02aecf9e327c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 19.5 Funktionsaufrufe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740652b-9dcf-4e88-ad79-9f3e96ce34db",
   "metadata": {},
   "source": [
    "Neuere KI-Systeme zeichnen sich durch sog. \"agentische\" Fähigkeiten aus, die durch den **Einsatz von Werkzeugen** (*tools*) erreicht werden. Hierbei definieren wir als Entwickler:innen die Werkzeuge meist als klassische (Python-)Funktionen, liefern bei jeder Anfrage an ein LLM eine Erklärung dieser Werkzeuge mit und verarbeiten die Antworten des LLM so, dass eine speziell formatierte Antwort einen Funktionsaufruf auslöst. Eine ausführlichere Einführung in das Thema findet sich [hier](https://huggingface.co/learn/agents-course/unit0/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69ec4c-44d6-45ef-b626-0ec072bb2753",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definition und Bereitstellung der Werkzeuge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297cc293-8fe8-4cf3-9684-ddcf84553bfc",
   "metadata": {},
   "source": [
    "Ein mittlerweile klassisches Beispiel für LLM-gesteuerte Funktionsaufrufe ist das Abrufen des Wetters an einem bestimmten Ort (s.a. Hausaufgabe 8). Eine solche Funktion ergänzt die Fähigkeiten eines LLM sehr gut, da diese keinen Zugriff auf aktuelle Informationen besitzen. Im folgenden Beispiel wird ´die Funktion `get_weather()` genutzt, die das Abrufen einer Wetter-API für drei festgelegte Orte simuliert.\n",
    "\n",
    "Damit das LLM versteht, welche Funktionen es nutzen kann, müssen wir diese in einem [speziellen Format](https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions) beschreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bbd5c-fe96-4663-8a0d-d40810353e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "model = \"ollama_chat/ministral-3:14b\"\n",
    "\n",
    "def get_weather(location):\n",
    "    weather_dict = {\n",
    "        \"frankfurt\": \"cloudy\",\n",
    "        \"tokyo\": \"sunny\",\n",
    "        \"new york\": \"rainy\"\n",
    "    }\n",
    "\n",
    "    return f\"The weather in {location} is {weather_dict.get(location.lower())}.\"\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get current weather for a given location.',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'location': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The name of the city for which to check the weather, e.g. New York'}},\n",
    "            'required': ['location']\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "chat = []\n",
    "chat.append({\"role\":\"user\", \"content\":\"What is the weather like in Tokyo?\"})\n",
    "\n",
    "res = completion(\n",
    "    model=model,\n",
    "    messages=chat,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "chat.append(res.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669e07e-9856-472a-8e8a-630d5590ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c85e9-cb00-4b76-bdc9-4410732f81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc6b36-d9b6-4709-acc9-c7439a1e03db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Verarbeiten der Antwort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76827fb0-449b-4cd2-80a9-9d33a564dfca",
   "metadata": {},
   "source": [
    "Wenn die Anfrage eines Users das LLM dazu bringt, einen Tool-Aufruf anzustoßen, gibt es eine speziell formatierte Antwort zurück. Diese müssen wir verarbeiten, indem wir die spezifizierten Funktionen aufrufen und das Ergebnis an das LLM zurückschicken. Auf der Grundlage dieser Informationen generiert es dann i.d.R. eine finale Antwort, die dem User angezeigt werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee2f09-13a8-4903-ac1d-27b60735d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "calls = res.choices[0].message.tool_calls\n",
    "for call in calls:\n",
    "    func_name = call.function.name\n",
    "    \n",
    "    if func_name == \"get_weather\":\n",
    "        args = json.loads(call.function.arguments)\n",
    "        result = get_weather(args.get(\"location\"))\n",
    "\n",
    "    else:\n",
    "        result = None\n",
    "\n",
    "    chat.append({\n",
    "        \"tool_call_id\": call.id,\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": func_name,\n",
    "        \"content\": str(result)\n",
    "    })\n",
    "\n",
    "res2 = completion(\n",
    "    model=model,\n",
    "    messages=chat,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d39176-3ea4-4a40-80e1-d44de5fc3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
